---
title: "p8105_hw2_yg2907"
author: "Yuandi Gao"
output: github_document
date: "2023-10-03"
---
```{r}
library(tidyverse)
library(readxl)
```

# Problem 1

Importing and cleaning the pols-month.csv dataset:

```{r cleaning pols-month, message = FALSE}
pols_df = read_csv("data/pols-month.csv") |> 
  janitor::clean_names() |> 
  separate(mon, into = c("year", "month", "day"), sep = "-") |> 
  mutate(
    year = as.numeric(year),
    month = month.name[as.numeric(month)],
    president = ifelse(
      prez_gop > prez_dem, "gop", "dem"
    )
  ) |> 
  select(-day, -prez_gop, -prez_dem)
```

Importing and cleaning the snp.csv dataset:

```{r cleaning snp, message = FALSE}
snp_df = 
  read_csv("data/snp.csv") |> 
  janitor::clean_names() |> 
  mutate(
    date = format(as.Date(date, format = "%m/%d/%y"), "%Y/%m/%d")
  ) |> 
  separate(date, into = c("year", "month", "day"), sep = "/") |> 
  mutate(
    month = month.name[as.numeric(month)],
    year = as.numeric(year),
    year = ifelse(year > 2023, year - 100, year)
  ) |> 
  select(-day)
```
Tidy the `unemployment.csv` 
```{r pivoting unemployment, message = FALSE}
unemployment_df = 
  read_csv("data/unemployment.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    jan:dec,
    names_to = "month",
    values_to = "unemployment"
  ) |> 
  mutate(
    year = as.numeric(year),
    month = month.name[match(month, tolower(month.abb))]
  )
```

Merging the 3 datasets
```{r merging, message = FALSE}
result_df = left_join(pols_df,snp_df, by = c("year", "month")) |> 
  left_join(unemployment_df, by = c("year", "month"))
```

## Problem 2

Read and clean the `Mr. Trash Wheel` sheet
* clean names and remove unnecessary rows
* Add trash wheel identifying name
* Recalculating homes powered 

```{r reading Mr Trash Wheel, message = FALSE}
trash_wheel_df = 
  readxl::read_excel("data/202309 Trash Wheel Collection Data.xlsx", 
                     sheet = "Mr. Trash Wheel", skip = 1) |> 
   janitor::clean_names() |>
  drop_na(dumpster) |> 
  select(-x15, -x16) |> 
  mutate(
    homes_powered = (weight_tons*500)/30,
    trash_wheel_name = "Mr. Trash Wheel",
    year = as.numeric(year)
  )
```

Perform similar cleaning as above 

```{r cleaning prof and gwynnda, message = FALSE}
gwynnda_trash_wheel_df = 
  readxl::read_excel("data/202309 Trash Wheel Collection Data.xlsx", 
                     sheet = "Gwynnda Trash Wheel", skip = 1) |> 
   janitor::clean_names() |>
  drop_na(dumpster) |> 
  mutate(
    homes_powered = (weight_tons*500)/30,
    trash_wheel_name = "Gwynnda Trash Wheel",
    year = as.numeric(year)
  )

prof_trash_wheel_df = 
  readxl::read_excel("data/202309 Trash Wheel Collection Data.xlsx", 
                     sheet = "Professor Trash Wheel", skip = 1) |> 
   janitor::clean_names() |>
  drop_na(dumpster) |> 
  mutate(
    homes_powered = (weight_tons*500)/30,
    trash_wheel_name = "Professor Trash Wheel",
    year = as.numeric(year)
  )
```
Combining three datasets
```{r merging trash wheel, message = FALSE}
trashwheel_master = bind_rows(trash_wheel_df, prof_trash_wheel_df, gwynnda_trash_wheel_df) |> select(trash_wheel_name = "trash_wheel_name", everything())
```
*The `Mr trash wheel` sheet has `r nrow(trash_wheel_df)` observations and `r ncol(trash_wheel_df)` columns. 
* The `Professor Trash Wheel` sheet has `r nrow(prof_trash_wheel_df)` observations and `r ncol(prof_trash_wheel_df)` columns. 
* The `Gwynnda Trash Wheel` sheet has `r nrow(gwynnda_trash_wheel_df)` observations and `r ncol(gwynnda_trash_wheel_df)` columns. 

These datasets record details of trash collected, with key variables such as dumpster id, date, weight, and litter type such as plastic bottles, cigarette butts, glass bottles, etc. There are missing data in `glass_bottles`, `sports balls` and `wrappers` in some column, meaning that these trash was not collected by those specific dumpsters. 

* The total weight of trash collected by Professor trash wheel is 
`r subset(trashwheel_master, trash_wheel_name == "Professor Trash Wheel") |> pull(weight_tons) |> sum()` tons.
* The total number of cigarette butts collected by Gwynnda in July of 2021 is `r subset(gwynnda_trash_wheel_df, year == 2021 & month == "July") |> pull(cigarette_butts) |> sum() |> as.integer()`.

### Problem 3

Import, clean, and tidy the dataset of baseline demographics. Ensure that sex and APOE4 carrier status are appropriate encoded (i.e. not numeric), and remove any participants who do not meet the stated inclusion criteria (i.e. no MCI at baseline). 

```{r reading baseline, message = FALSE}
baseline = 
  read_csv("data/MCI_baseline.csv", skip = 1) |> 
  janitor::clean_names() |> 
  mutate(
    sex = case_match(
      sex,
      1 ~ "male",
      0 ~ "female"
    ),
    apoe4 = case_match(
      apoe4,
      1 ~ "carrier",
      0 ~ "non-carrier"
    )
  ) |> 
  filter(age_at_onset == "." | age_at_onset > current_age)
```

The important steps in importing this dataset:
- Recode binary variables and remove ineligible participants by removing all records whose onset for MCI is earlier or at the same age at baseline. `r 483 - nrow(baseline)` participants were removed, the demographics information at baseline is complete. 
- `r nrow(baseline)` participants were recruited at baseline. 
- `r filter(baseline, age_at_onset != ".") |> nrow()` participants developed MCI during the study.
- The average baseline age is `r mean(pull(baseline, current_age)) |> round(digits = 0)`.
- Proportion of women who are APOE 4 carrier is 
`r scales::percent(nrow(filter(baseline, sex == "female" & apoe4 =="carrier")) / nrow(filter(baseline, sex == "female")))`.

Similarly, import, clean, and tidy the dataset of longitudinally observed biomarker values; comment on the steps on the import process and the features of the dataset.
```{r cleaning amyloid, message = FALSE}
amyloid = 
  read_csv("data/mci_amyloid.csv", skip = 1) |> 
  janitor::clean_names() |> 
  rename("id" = "study_id") 
```

Pivot `amyloid` from wide to long format
```{r}
amyloid_long = amyloid |> pivot_longer(
  baseline:time_8,
  names_to = "visit",
  values_to = "amyloid_ratio"
)
```
Steps on the import process and the features of the dataset.
- Besides from cleaning names, the study_id has to be renamed to match the `baseline` dataset to proceed with merge. 
- There are `r nrow(amyloid)` participants in the longitudinal dataset, and MCI is recorded at baseline, time 2, time 4, time 6, and time 8. 
- There are a significant amount of missing values meaning that many participants missed visits. 

Merge the amyloid dataset with baseline, keeping participants in either sets
```{r merge full, message = FALSE}
amyloid_full = 
  full_join(baseline, amyloid, by = c("id"))
```
- There are `r nrow(amyloid_full) - nrow(baseline)` participants in the amyloid set but not in the baseline set. 
- There are `r nrow(amyloid_full) - nrow(amyloid)` participants in the baseline set but not in the amyloid set. 

Merge the amyloid long format dataset with baseline to keep those in both datasets
```{r merge only in both, message = FALSE}
amyloid_both = 
  inner_join(baseline, amyloid_long, by = c("id"))
```

- There are `r n_distinct(pull(amyloid_both, id))` participants that are in both the baseline and longitudinal visit datasets. 
- There are `r nrow(amyloid_both)` rows and `r ncol(amyloid_both)` columns in the merged dataset. 

Exporting the result dataframe to a csv file 
```{r export to csv, message = FALSE}
write.csv(amyloid_both, "data/baseline_amyloid_merged.csv", row.names = FALSE)
```